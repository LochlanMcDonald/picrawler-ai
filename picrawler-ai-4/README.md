# PiCrawler-AI v4 - Robust Layered Architecture

**Complete redesign** using industry-standard robotics architecture principles.

## What's New in v4

### âœ… Sensor Fusion (WorldModel)
- **Before**: AI and sensors conflicted
- **After**: Sensors fused into unified world model BEFORE AI sees it
- **Result**: AI makes informed decisions with complete picture

### âœ… Spatial Memory (Learning)
- **Before**: No memory, repeated same failures
- **After**: Learns from history, avoids failed strategies
- **Result**: Gets smarter over time, natural stuck detection

### âœ… Behavior Trees (Structured Decisions)
- **Before**: Linear flow with random overrides
- **After**: Hierarchical priorities with fallbacks
- **Result**: Predictable, testable behavior

### âœ… Layered Control
- **Before**: AI tried to control motors directly
- **After**: Fast reactive â†’ Behavior tree â†’ AI strategy
- **Result**: Safe, responsive, intelligent

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AI Planner (5s)       â”‚  â† High-level strategy
â”‚   Multi-step plans      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Behavior Tree (500ms)  â”‚  â† Action selection
â”‚  Structured fallbacks   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WorldModel (50ms)      â”‚  â† Sensor fusion
â”‚  Ultrasonic + Vision    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
         ðŸ¤– ROBOT
```

## Quick Start

```bash
# Setup (first time)
cd picrawler-ai-4
cp config/config.example.json config/config.json
nano config/config.json  # Add your OpenAI API key

# Run standard exploration
python main.py --mode explore --duration 5 --verbose

# Run cautious mode (shorter movements, more careful)
python main.py --mode cautious --duration 5

# Test (one frame + analysis)
python main.py --mode test
```

## Directory Structure

```
picrawler-ai-4/
â”œâ”€â”€ core/                  # Core infrastructure
â”‚   â”œâ”€â”€ world_model.py         # Sensor fusion
â”‚   â”œâ”€â”€ spatial_memory.py      # History & learning
â”‚   â””â”€â”€ robot_controller.py    # Hardware abstraction
â”‚
â”œâ”€â”€ perception/            # Sensing & understanding
â”‚   â”œâ”€â”€ camera.py              # Camera capture
â”‚   â””â”€â”€ vision_ai.py           # Scene analysis
â”‚
â”œâ”€â”€ planning/              # Decision making
â”‚   â”œâ”€â”€ behavior_tree.py       # Behavior tree framework
â”‚   â””â”€â”€ ai_planner.py          # High-level strategy
â”‚
â”œâ”€â”€ config/                # Configuration
â”‚   â””â”€â”€ config.example.json    # Template config
â”‚
â”œâ”€â”€ logs/                  # Output
â”‚   â””â”€â”€ operation_v4.log       # Runtime logs
â”‚
â””â”€â”€ main.py                # Main control loop
```

## Key Components

### WorldModel (core/world_model.py)
**Purpose**: Unified sensor fusion

**Features**:
- Combines ultrasonic + vision into single view
- Obstacle detection in 4 directions
- Suggests best direction based on all sensors
- Calculates free space score (0=trapped, 1=open)

**Usage**:
```python
world_model = WorldModel(obstacle_threshold_cm=20)
world_model.update_ultrasonic(15.5)  # Physical sensor
world_model.update_vision(objects, hazards, description)  # Vision AI
is_safe = world_model.is_safe_to_move('forward')
```

---

### SpatialMemory (core/spatial_memory.py)
**Purpose**: Learn from experience

**Features**:
- Tracks action history and outcomes
- Learns which directions work better
- Detects stuck patterns:
  - Same action repeating
  - Oscillation (left-right-left-right)
  - No spatial progress
  - Failed escape attempts
- Suggests action scores based on history

**Usage**:
```python
memory = SpatialMemory()
memory.record_action('forward', success=False, reason="blocked")
is_stuck = memory.is_stuck()
best_turn = memory.get_best_turn_direction()
```

---

### Behavior Trees (planning/behavior_tree.py)
**Purpose**: Structured decision making

**Features**:
- SequenceNode: Do A, then B, then C (fail if any fail)
- FallbackNode: Try A, if fails try B, if fails try C
- Pre-built exploration trees
- Composable, testable behaviors

**Tree Structure**:
```
Exploration Root (Fallback)
â”œâ”€ Stuck Recovery (Sequence)
â”‚  â”œâ”€ Check if stuck
â”‚  â””â”€ Execute recovery
â”œâ”€ Move Forward (Sequence)
â”‚  â”œâ”€ Check path clear
â”‚  â””â”€ Move forward
â”œâ”€ Find Alternative (Sequence)
â”‚  â”œâ”€ Smart turn (uses memory)
â”‚  â”œâ”€ Check path clear
â”‚  â””â”€ Move forward
â””â”€ Aggressive Maneuver (Sequence)
   â”œâ”€ Back up
   â”œâ”€ Smart turn
   â””â”€ Move forward
```

---

### AI Planner (planning/ai_planner.py)
**Purpose**: High-level strategy (not motor control)

**Input**:
- Full world model state
- Complete action history
- Memory statistics

**Output**:
```json
{
  "primary": ["turn_left", "forward", "forward"],
  "fallback": ["turn_right", "forward"],
  "recovery": ["backward", "turn_left"],
  "reasoning": "Front blocked, left has more clearance",
  "confidence": 0.85
}
```

**When Used**:
- Every 5 seconds
- After 2+ consecutive failures
- Can request replanning anytime

---

## How It Works

### Control Loop (60ms cycle)

```python
while running:
    # 1. PERCEPTION (Fast - 50ms)
    distance = robot.get_distance()        # Ultrasonic
    world_model.update_ultrasonic(distance)

    # Every 2.5s:
    image = camera.capture()               # Camera
    analysis = vision_ai.analyze(image)   # Vision AI
    world_model.update_vision(analysis)   # Fuse

    # 2. BEHAVIOR (Medium - 500ms)
    context = BehaviorContext(world_model, memory, robot)
    status = behavior_tree.execute(context)

    # 3. PLANNING (Slow - 5s, when needed)
    if should_replan():
        plan = ai_planner.plan(world_model, memory)

    # 4. LEARNING
    memory.record_outcomes()
```

### Decision Flow

```
Sensor reads 15cm obstacle
    â†“
WorldModel: "Front blocked (ultrasonic, high confidence)"
    â†“
BehaviorTree: "Path not clear, try alternative"
    â†“
SmartTurn: Memory says right worked better
    â†“
Execute: turn_right 0.9s
    â†“
Memory: Record turn_right success
```

## Differences from v3

| Aspect | v3 (Old) | v4 (New) |
|--------|----------|----------|
| **Decision** | AI â†’ Override | Sense â†’ Plan â†’ Act |
| **Sensors** | Separate streams | Fused into WorldModel |
| **Actions** | Single action | Multi-step plans |
| **Fallbacks** | Random override | Behavior tree priorities |
| **Memory** | None | SpatialMemory learning |
| **Stuck** | Counter > 3 | Pattern recognition |
| **AI Role** | Motor control | High-level strategy |
| **Testability** | Hard (monolithic) | Easy (modular) |

## Testing

### Test Sensor Fusion
```python
from core.world_model import WorldModel

world = WorldModel()
world.update_ultrasonic(15.0)  # Close obstacle
world.update_vision([], ['wall'], "Wall ahead")

print(world.is_safe_to_move('forward'))  # False
print(world.get_best_direction())        # 'turn_left' or 'turn_right'
```

### Test Spatial Memory
```python
from core.spatial_memory import SpatialMemory

memory = SpatialMemory()

# Simulate stuck pattern
for _ in range(10):
    memory.record_action('forward', success=False, reason="blocked")

print(memory.is_stuck())  # True
```

### Test Behavior Tree
```python
from planning.behavior_tree import build_exploration_tree, BehaviorContext

tree = build_exploration_tree()
context = BehaviorContext(world_model, memory, robot)
status = tree.execute(context)
```

## Configuration

**Obstacle Threshold**:
```json
"obstacle_distance_threshold_cm": 20  // Stop if < 20cm
```
- Lower (10-15): More aggressive
- Higher (25-30): More cautious

**Camera Interval**:
```json
"capture_interval_s": 2.5  // Update vision every 2.5s
```
- Lower (1.0): More responsive, more API calls
- Higher (5.0): Less responsive, fewer API calls

## Expected Performance

- **Stuck incidents**: Rare (behavior tree + memory prevent loops)
- **Escape success**: ~90% (proper recovery sequences)
- **Coverage**: High (efficient exploration)
- **Decision latency**: 500ms (behavioral) + 5s (replanning)

## Migration from v3

v3 still works! v4 is a complete redesign.

**To migrate**:
1. Keep v3 as backup
2. Copy config to v4
3. Test v4 in parallel
4. Compare results

**v4 is better if you want**:
- Fewer stuck situations
- Learning from experience
- More predictable behavior
- Easier to debug/test

## Troubleshooting

**Robot gets stuck**:
- Check logs for stuck detection
- Memory should trigger recovery
- Verify sensor fusion is working

**AI not used**:
- Behavior tree handles most decisions
- AI only used for strategic replanning
- This is intentional (faster, more reliable)

**Sensor fusion not working**:
- Check `world_model.obstacles['front']`
- Should show distance and confidence
- Both ultrasonic and vision should contribute

## Future Enhancements

- [ ] SLAM / position tracking
- [ ] Multi-robot coordination
- [ ] Web dashboard for monitoring
- [ ] Replay system from logs
- [ ] More behavior trees (search, mapping, etc.)

---

## Philosophy

**v3**: "AI controls everything, override when wrong"
**v4**: "Sensors â†’ Behavior â†’ AI strategy, each layer does what it's good at"

The robot is now **collaborative** (layers work together) not **combative** (layers fight each other).
